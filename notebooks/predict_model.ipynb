{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'has_changed_dir' not in globals():\n",
    "    repo_path = os.path.abspath(os.path.join('..'))\n",
    "    \n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    \n",
    "    os.chdir(repo_path)\n",
    "    \n",
    "    globals()['has_changed_dir'] = True\n",
    "    print(repo_path)\n",
    "    print(os.getcwd())\n",
    "\n",
    "\n",
    "# from typing import Optional, Union\n",
    "\n",
    "# import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from src.train_model import LightningModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model using ckpt (you dont need it if you convert your model to transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path: str = \"models/re-init/\"\n",
    "# tokenizer_path: str = \"models/re-init/\"\n",
    "# tokenizer_vocab_file_path: str = \"models/re-init/sentencepiece.bpe.model\"\n",
    "# ckpt_path: Optional[Union[str, None]] = \"models/it_1/epoch=17-val_loss=1.48184.ckpt\"\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
    "# model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, vocab_file = tokenizer_vocab_file_path)\n",
    "\n",
    "\n",
    "# # Option 1:\n",
    "# # lightning_model = LightningModel.load_from_checkpoint(ckpt_path, map_location=torch.device(\"cuda:1\"), model = model, tokenizer = tokenizer)\n",
    "\n",
    "# # Option 2:\n",
    "# ckpt = torch.load(ckpt_path, map_location=torch.device(\"cuda:1\"))\n",
    "# lightning_model = LightningModel(model, tokenizer)\n",
    "# lightning_model.load_state_dict(ckpt['state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/home/raki/raki-projects/t5-tuning/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load local converted model\"\"\"\n",
    "\n",
    "# final_model_path = \"models/final\"\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(final_model_path, vocab_file = final_model_path + \"/sentencepiece.bpe.model\")\n",
    "# lightning_model = LightningModel(model, tokenizer)\n",
    "\n",
    "\n",
    "\"\"\" Download and load converted model from hub \"\"\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"leks-forever/mt5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"leks-forever/mt5-base\")\n",
    "lightning_model = LightningModel(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefixes:    \n",
    "`\"translate Russian to Lezghian: \"` - Ru-Lez    \n",
    "`\"translate Lezghian to Russian: \"` - Lez-Ru    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Когда римские воины и вожди, а также главные священнослужители и блюстители Закона пришли в Иудею, они дали ему вооружённые оружие, браслеты и серьги.']\n"
     ]
    }
   ],
   "source": [
    "sentence: str = \"Римдин аскерар ва гьакӀни чӀехи хахамрини  фарисейри ракъурнавай нуькерар Ягьуд галаз багъдиз атана. Абурув виридав яракьар, чирагъар ва шемгьалар гвай.\"\n",
    "\n",
    "translation = lightning_model.predict(sentence, prefix=\"translate Lezghian to Russian: \")\n",
    "\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['И гафар ван хьайила, Исади лагьана: - Жерягь сагъбуруз ваъ, начагъбуруз герек я.']\n"
     ]
    }
   ],
   "source": [
    "sentence: str = \"Когда Исо услышал это, Он сказал: – Не здоровым нужен врач, а больным.\"\n",
    "\n",
    "translation = lightning_model.predict(sentence, prefix=\"translate Russian to Lezghian: \")\n",
    "\n",
    "print(translation)\n",
    "\n",
    "# Реальный перевод: И гафар ван хьайила, Исади абуруз лагьана: – Жерягь сагъбуруз ваъ, начагъбуруз герек я."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Куьне тӀили фу неъ, ахпа са тӀимил кьван ша чун.']\n",
      "['КӀвализ гьахьайла, рагъ экъечӀдай вахтунда цӀайлапанар авай ва вири юкьвал мичӀи хьанвай.']\n",
      "['Чна акур чаз акур цӀийи кьилелай зун гзаф хъел атана']\n",
      "['Пакадин йисуз за а чкадиз абурун халкьдин гъавурда чир хьун патал са бязи уьлкведиз къвез кӀанзава.']\n",
      "['Гьар са кӀвалахдин йикъан няниз хъфидай югъ алукьнавай кӀвалахни хъваз шадвал ацукь.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Давай поедим пиццу, а потом еще немного погуляем!\",\n",
    "    \"Я люблю гулять по парку ранним утром, когда воздух свежий и тишина вокруг.\",\n",
    "    \"Новый фильм, который мы смотрели вчера, произвёл на меня сильное впечатление\",\n",
    "    \"В следующем году я планирую посетить несколько стран, чтобы познакомиться с их культурой.\",\n",
    "    \"После долгого рабочего дня приятно расслабиться с книгой и чашкой чая.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    translation = lightning_model.predict(sentence, prefix=\"translate Russian to Lezghian: \")\n",
    "\n",
    "    print(translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "You can use the predict method regardless of the Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, prefix, a=32, b=3, max_input_length=1024, num_beams=1, **kwargs):\n",
    "    inputs = tokenizer(prefix + text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n",
    "    result = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        num_beams=num_beams,\n",
    "        **kwargs\n",
    "    )\n",
    "    return tokenizer.batch_decode(result, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Когда римские воины и вожди, а также главные священнослужители и блюстители Закона пришли в Иудею, они дали ему вооружённые оружие, браслеты и серьги.']\n"
     ]
    }
   ],
   "source": [
    "sentence: str = \"Римдин аскерар ва гьакӀни чӀехи хахамрини  фарисейри ракъурнавай нуькерар Ягьуд галаз багъдиз атана. Абурув виридав яракьар, чирагъар ва шемгьалар гвай.\"\n",
    "\n",
    "translation = predict(sentence, prefix=\"translate Lezghian to Russian: \")\n",
    "\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
